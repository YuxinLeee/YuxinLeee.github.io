<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="assets/css/style.css" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-DPH2YST3Z6"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DPH2YST3Z6');
    </script>

    <title> Yuxin Li </title>
    <!-- <link rel="icon" type="image/x-icon" href="assets/images/stanford.ico"> -->
</head>

<body>
    <div style="width:1000px;margin: 0px auto;">
        <header id="header" width="400px" style="display:flex;justify-content: space-around;">
            <a href="#profile-intro">Home</a>
            <a href="#updates">News</a>
            <a href="#research">Research</a>
            <a href="#blog">Blog</a>
            <a href="#talks">Talks</a>
            <a href="#service">Service</a>
        </header>
        <div id="profile">
            <div id="profile-pic">
                <img src="figs/headshot_mid.jpg" />
            </div>
            <div id="profile-intro">
                <div id="profile-name">Yuxin Li ÊùéÂÆáÊ¨£</div>
                <div id="profile-email">yuxinlee@gmail.com</div>
                <p>
I am Yuxin Li, a recent graduate from GRASP Lab, UPenn where I completed my master's degree under Prof. Nadia Figueroa's supervision. 
                    <br>
                    <br>
My background is primarily in developing safe and efficient robotic planning and control algorithms in human-centric environmentsÔºåspecializing in the areas of robot manipulation, control, planning and learning, with a particular focus on motion strategy transfer in dynamic systems. I expect to further explore the adaptive and generalization capabilities of robots in complex environments, and work on advancing the design of intelligent systems for robot-human interactions to make robots smarter to assist humans.                 </p>
                <div>
                    <a href="cv/cv_xxm.pdf">
                        CV
                    </a>
                    /
                    <a href="https://scholar.google.com/citations?hl=en&user=af_4iHYAAAAJ">
                        GScholar
                    </a>
                    /
                    <a href="https://twitter.com/XiaomengXu11">
                        Twitter
                    </a>
                    /
                    <a href="https://github.com/xxm19">
                        Github
                    </a>
                    /
                    <a href="https://www.linkedin.com/in/xiaomeng-xu-449b6825a">
                        LinkedIn
                    </a>
                    </a>
                </div>
            </div>
            <div style="clear: both;"></div>
        </div>
        <!-- <div class="section" id="updates">
            <h1>News</h1>
            <ul>
                <li> <b>Sep 2025</b> <a href="https://compliant-residual-dagger.github.io/">Compliant Residual DAgger</a> is accepted to Neurips 2025 and won Best Paper at the <a href="https://sites.google.com/view/h2r-corl2025/home">Human-to-Robot Workshop @ CoRL 2025</a>!</li>
                <li> <b>Jun 2025</b> Research intern at <a href="https://www.tri.global/our-work/large-behavior-models">TRI LBM</a> team in Cambridge, MA.</li>
                <li> <b>May 2025</b> Grateful to be awarded the <a href="https://vpge.stanford.edu/fellowships-funding/sigf">Stanford Interdisciplinary Graduate Fellowship</a>.</li>
                <li> <b>Apr 2025</b> <a href="https://robopanoptes.github.io/">RoboPanoptes</a> is accepted to RSS 2025, see you in LA! üêç</li>
                <li> <b>Nov 2024</b> <a href="https://dgdm-robot.github.io">Dynamics-Guided Diffusion Model</a> won Best Machine Learning Paper at the <a href="https://sites.google.com/view/corl-mapodel-workshop">MAPoDeL Workshop @ CoRL 2024</a>!</li>
                <li> <b>Sep 2024</b> <a href="https://dgdm-robot.github.io">Dynamics-Guided Diffusion Model</a> is accepted to CoRL 2024. üéâ</li>
                <li> <b>Sep 2023</b> Started PhD at Stanford University. üå≤</li>
                <li> <b>Jun 2023</b> Graduated from Tsinghua University with the highest honor. üéì</li>
                <li> <b>Jun 2022</b> Research intern at Stanford through the <a href="https://engineering.stanford.edu/students-academics/programs/global-engineering-programs/chinese-ugvr">UGVR program</a> advised by <a href="http://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas.</li>
            </ul>
            <p></p>
            <div style="clear: both;"></div>
        </div> -->
        <div class="divider"></div>
        <div style="display:flex;flex-direction: column;" id="research">
            <h1>Research</h1>
            

            <div>
                <a href="https://arxiv.org/abs/2210.04026" style="height: 12em;" class="research-thumb">
                    <video id="teaser" autoplay muted loop width="100%">
                        <source src="figs/tegtrack.mp4"
                                type="video/mp4">
                    </video>
                </a>
                <a href="https://arxiv.org/abs/2210.04026" class="research-proj-title">
                    Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing
                </a>
                <p>
                    Yun Liu*,
                    <b>Xiaomeng Xu*</b>,
                    Weihang Chen, Haocheng Yuan,
                    <a style="color:#000;" href="https://hughw19.github.io/">He Wang</a>, Jing Xu,
                    <a style="color:#000;" href="https://cray695.wixsite.com/mysite">Rui Chen</a>,
                    <a style="color:#000;" href="https://ericyi.github.io/">Li Yi</a>
                    <br>Robotics and Automation Letters (RA-L 2023) with a presentation at ICRA 2024<br>
                    <a href="https://arxiv.org/abs/2210.04026">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://www.youtube.com/watch?v=S8lnBQhsfHk">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/leolyliu/TEG-Track">Code</a>
                </p>
                <br>
                <p>
                    <b>TL;DR</b>: Tactile-enhanced 6D pose tracking of previously unseen in-hand objects.
                </p>
            </div>

            <!-- <div style="clear: both;"></div> -->
        </div>
        <div>
            <p>
                <!-- * equal contribution. -->
            </p>
        </div>
        <div class="divider"></div>
        <div class="section" id="blog">
            <h1>Blog</h1>
            <div>
                <a href="https://ai.stanford.edu/blog/robopanoptes/" style="height: 12em;" class="research-thumb">
                    <img src="figs/all robots.png" alt="Blog thumbnail" style="width: 100%; height: auto;">
                </a>
                <a href="https://ai.stanford.edu/blog/robopanoptes/" class="research-proj-title">
                    How Machine Learning Brings Unconventional Robots to Life
                </a>
                <p>
                    Robot learning has largely focused on standard platforms‚Äîbut can it embrace robots of all shapes and sizes?
                    In this blog post, we show how data-driven methods bring unconventional robots to life, enabling capabilities that traditional designs and control can't achieve.
                    <br><br>
                    <a href="https://ai.stanford.edu/blog/robopanoptes/" style="display:inline-block; margin-top: 8px; padding: 6px 12px; background-color: #AFDDFF; color: white; text-decoration: none; border-radius: 4px; font-size: 1em;">
                    <b>Continue reading ‚Üí</b>
                    </a>
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div>

    

        <div class="section" id="service">
            <h1>Service</h1>
            <ul>
                <li> Teaching assistant of <a href="https://sites.google.com/view/aut24-25-eecs-227/home">EE/CS227: Robot Perception</a></li>
                <li> Co-organizer of <a href="https://rss-hardware-intelligence.github.io/">RSS 2025 1st Workshop on Robot Hardware-Aware Intelligence</a></li>
                <li> Reviewer of RSS 2025, CoRL 2025, T-RO 2025, RA-L 2025, ICRA 2024, IROS 2025, Neurips 2024-2025, ICLR 2025, ICML 2025, AAAI 2025, CVPR 2023
                <!-- <li> Drop-in tutor for engineering courses at Tsinghua University. -->
            </ul><br>
            <p></p>
            <div style="clear: both;"></div>
        </div>
    </div>

</body>

</html>
